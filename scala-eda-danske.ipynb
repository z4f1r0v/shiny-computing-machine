{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danske Bank account exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "implicit val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.time.LocalDateTime\n",
    "import java.time.format.DateTimeFormatter\n",
    "import org.apache.spark.sql.functions.udf\n",
    "                                       \n",
    "val updateTimestamp = udf((timestamp: String, index: Long) => {\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")\n",
    "    val dateTime = LocalDateTime.parse(timestamp, formatter)\n",
    "    dateTime.plusSeconds(index).toString\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def withCleanColumnNames(df: DataFrame)(implicit spark: SparkSession): DataFrame =\n",
    "    df.columns.foldLeft(df)((df, col) =>\n",
    "            df.withColumnRenamed(\n",
    "                existingName = col,\n",
    "                newName = col.replaceAll(\"Ã¸\", \"o\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def withFloat(columnName: String, df: DataFrame)(implicit spark: SparkSession): DataFrame =\n",
    "    df.withColumn(columnName, regexp_replace(col(columnName), \"\\\\.\", \"\"))\n",
    "      .withColumn(columnName, regexp_replace(col(columnName), \",\", \".\"))\n",
    "      .withColumn(columnName, col(columnName).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{LongType, StructField, StructType}\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "\n",
    "def withColumnIndex(df: DataFrame)(implicit spark: SparkSession): DataFrame = {\n",
    "    spark.sqlContext.createDataFrame(\n",
    "      df.rdd.zipWithIndex.map {\n",
    "        case (row, index) => Row.fromSeq(row.toSeq :+ index)\n",
    "      },\n",
    "      // Create schema for index column\n",
    "      StructType(df.schema.fields :+ StructField(\"index\", LongType, false)))\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val dankortDF = spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"encoding\", \"iso-8859-1\")\n",
    "      .csv(\"dankort.csv\")\n",
    "\n",
    "val withIndex = dankortDF.transform(withCleanColumnNames)\n",
    "    .transform(withColumnIndex)\n",
    "\n",
    "val transformedDF = withIndex\n",
    "    .withColumn(\"Dato\", to_timestamp(col(\"Dato\"), \"dd.MM.yyyy\"))\n",
    "    .withColumn(\"Dato\", updateTimestamp(col(\"Dato\"), col(\"index\")))\n",
    "    .filter(col(\"Dato\").gt(\"2019-05-26\"))\n",
    "    .transform(df => withFloat(\"Saldo\", df))\n",
    "    .transform(df => withFloat(\"Belob\", df))\n",
    "    .sort(\"Dato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.1`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "// if you want to have the plots available without an internet connection:\n",
    "init(offline=true)\n",
    "\n",
    "// restrict the output height to avoid scrolling in output cells\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the trend for the given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val (x, y) = transformedDF.select(\"Dato\", \"Saldo\")\n",
    "    .collect.map(r=>(r(0).toString, r(1).toString.toDouble)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which is the most expensive transaction for each month of each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dankortDateDF = transformedDF.withColumn(\"Dato\", to_date(col(\"Dato\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dankortDateDF.createOrReplaceTempView(\"dankort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select Dato, Belob, Tekst from (\n",
    "    select *, row_number() OVER (PARTITION BY (year(Dato), month(Dato)) ORDER BY Belob asc) as rn from dankort\n",
    "    ) tmp where rn = 1\n",
    " \"\"\").show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
